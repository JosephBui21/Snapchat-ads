{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snapchat Political Ads\n",
    "This project uses political ads data from Snapchat, a popular social media app. Interesting questions to consider include:\n",
    "- What are the most prevalent organizations, advertisers, and ballot candidates in the data? Do you recognize any?\n",
    "- What are the characteristics of ads with a large reach, i.e., many views? What may a campaign consider when maximizing an ad's reach?\n",
    "- What are the characteristics of ads with a smaller reach, i.e., less views? Aside from funding constraints, why might a campaign want to produce an ad with a smaller but more targeted reach?\n",
    "- What are the characteristics of the most expensive ads? If a campaign is limited on advertising funds, what type of ad may the campaign consider?\n",
    "- What groups or regions are targeted frequently? (For example, for single-gender campaigns, are men or women targeted more frequently?) What groups or regions are targeted less frequently? Why? Does this depend on the type of campaign?\n",
    "- Have the characteristics of ads changed over time (e.g. over the past year)?\n",
    "- When is the most common local time of day for an ad's start date? What about the most common day of week? (Make sure to account for time zones for both questions.)\n",
    "\n",
    "### Getting the Data\n",
    "The data and its corresponding data dictionary is downloadable [here](https://www.snap.com/en-US/political-ads/). Download both the 2018 CSV and the 2019 CSV. \n",
    "\n",
    "The CSVs have the same filename; rename the CSVs as needed.\n",
    "\n",
    "Note that the CSVs have the exact same columns and the exact same data dictionaries (`readme.txt`).\n",
    "\n",
    "### Cleaning and EDA\n",
    "- Concatenate the 2018 CSV and the 2019 CSV into one DataFrame so that we have data from both years.\n",
    "- Clean the data.\n",
    "    - Convert `StartDate` and `EndDate` into datetime. Make sure the datetimes are in the correct time zone.\n",
    "- Understand the data in ways relevant to your question using univariate and bivariate analysis of the data as well as aggregations.\n",
    "\n",
    "*Hint 1: What is the \"Z\" at the end of each timestamp?*\n",
    "\n",
    "*Hint 2: `pd.to_datetime` will be useful here. `Series.dt.tz_convert` will be useful if a change in time zone is needed.*\n",
    "\n",
    "*Tip: To visualize geospatial data, consider [Folium](https://python-visualization.github.io/folium/) or another geospatial plotting library.*\n",
    "\n",
    "### Assessment of Missingness\n",
    "Many columns which have `NaN` values may not actually have missing data. How come? In some cases, a null or empty value corresponds to an actual, meaningful value. For example, `readme.txt` states the following about `Gender`:\n",
    "\n",
    ">  Gender - Gender targeting criteria used in the Ad. If empty, then it is targeting all genders\n",
    "\n",
    "In this scenario, an empty `Gender` value (which is read in as `NaN` in pandas) corresponds to \"all genders\".\n",
    "\n",
    "- Refer to the data dictionary to determine which columns do **not** belong to the scenario above. Assess the missingness of one of these columns.\n",
    "\n",
    "### Hypothesis Test / Permutation Test\n",
    "Find a hypothesis test or permutation test to perform. You can use the questions at the top of the notebook for inspiration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of Findings\n",
    "\n",
    "### Introduction\n",
    "My partner and I have chosen to work on the political ads data(2018 & 2019) from Snapchat, a popular social media app. What we plan to achieve in this project is to clean the data using concise and clear code, assess the missingness of data and fill those missing values based on its context. Lastly, we will perform a hypothesis test based on the question: What are the characteristics of the most expensive ads? If a campaign is limited on advertising funds, what type of ad may the campaign consider?\n",
    "\n",
    "### Cleaning and EDA\n",
    "First, we downloaded the 2018 and 2019 datasets from the snapchat website where we would convert the Comma-Separated Values(CSV) into dataframes using pandas. Next, we concatenated the data from both years into one dataframe where we cleaned the data by changing the 'StartDate' and 'EndDate' into a consistent time zone (UTC) and a common date time in pandas. We converted each column that would be considered a choice by the advertiser. To clean our data, we made new columns for each columns that were Agnostic or not so that we can test for our hypothesis test later. The Exploratory Data Analysis we used was a Bivariate analysis where we took the mean of the Impressions / Spend and normalized the data so that it will be more ideal for us when running our hypothesis tests.\n",
    "\n",
    "### Assessment of Missingness\n",
    "We ran a permutation test to determine if gender missingness (NaN = both genders) is dependent on how much a company spends on an advertisement compared to companies who only target single genders (female or male). We concluded that gender missingness is not dependent on how much a company spends. We also ran a permutation test to determine if segment missingness is dependent on how much a company spends on an advertisement compared to companies who have a particular segment. We concluded that segment missingness is dependent on how much a company spends.\n",
    "\n",
    "### Hypothesis Test\n",
    "Test Statistic Used: Difference of Means\n",
    "Null Hypothesis: Whether or not ads are targeted agnostically(by column) or specifically will not affect the impressions per dollar spent by the advertiser\n",
    "Alternative Hypothesis: Having an ad be targeted at specific types of users will increase the number of impressions per dollar spent\n",
    "We FAILED to reject the null hypothesis using the significance level of 0.05, tested columns(all failed to reject): Language(Involving English, or Agnostic), Interests, Ages, LatLongRad, Affiliation(Candidate/Ballot Info), District, OSType, Segments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.652554Z",
     "start_time": "2019-10-31T23:36:27.180520Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import datetime as dt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'  # Higher resolution figures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning and EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.657068Z",
     "start_time": "2019-10-31T23:36:28.654650Z"
    }
   },
   "outputs": [],
   "source": [
    "fp_2018 = os.path.join('PoliticalAds18', 'PoliticalAds2018.csv')\n",
    "fp_2019 = os.path.join('PoliticalAds19', 'PoliticalAds2019.csv')\n",
    "data_2018 = pd.read_csv(fp_2018)\n",
    "data_2019 = pd.read_csv(fp_2019)\n",
    "bofa = pd.concat([data_2018, data_2019], ignore_index=True)\n",
    "bofa['StartDate'] = pd.to_datetime(bofa['StartDate'])\n",
    "bofa['EndDate'] = pd.to_datetime(bofa['EndDate'])\n",
    "#we \n",
    "bofa['Gender'] = bofa['Gender'].fillna('BOTH').replace(['MALE', 'FEMALE'], 'SINGLE')\n",
    "#we made a column within the dataframe that keeps track of Impressions per Dollar spent for each ad\n",
    "bofa['ImpPerDoll'] = bofa['Impressions'] / bofa['Spend']\n",
    "#we replaced 'inf' with NaN values as some ads did not spend any money which means it was funded by Snapchat\n",
    "bofa['ImpPerDoll'] = bofa['ImpPerDoll'].replace([np.inf, -np.inf], np.NaN)\n",
    "#we normalized the Impressions per Dollar\n",
    "bofa['ImpPerDoll'] = (bofa['ImpPerDoll'] - bofa['ImpPerDoll'].mean()) / bofa['ImpPerDoll'].std()\n",
    "bofa['Agnostic'] = bofa['Language'].fillna('Agnostic') == 'Agnostic' #Any Languages vs Specific Languages\n",
    "bofa['English'] = bofa['Language'].str.contains('en').fillna(True) #Contains english vs Does not \n",
    "bofa['AllOsTypes'] = bofa['OsType'].fillna('Agnostic') == 'Agnostic' #Specific OS types vs All OS\n",
    "bofa['All_Interests'] = bofa['Interests'].fillna('Agnostic') == 'Agnostic' #All Interests vs Specified Interests\n",
    "bofa['All_Ages'] = bofa['AgeBracket'].fillna('Agnostic') == 'Agnostic' #All Ages vs Specified Age Range\n",
    "bofa['All_Lat'] = bofa['LatLongRad'].fillna('Agnostic') == 'Agnostic'#Any LongLatRad vs Specified LongLatRad\n",
    "#Any Affiliation vs Specific Candidate/Ballot\n",
    "bofa['Unaffiliated'] = bofa['CandidateBallotInformation'].fillna('Unaffiliated') == 'Unaffiliated' \n",
    "bofa['All_Dist'] = bofa['ElectoralDistrictID'].fillna('All') == 'All' #Any Dist vs Specific Dist of County"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "bofa['Seg'] = bofa['Segments'] == 'Provided by Advertiser' #Provided by Advertiser vs Not provided by advertiser\n",
    "bofa['AdvancedDemographics'] = bofa['AdvancedDemographics'].fillna('Agnostic') == 'Agnostic' #Specified Demo vs All Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessment of Missingness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.662099Z",
     "start_time": "2019-10-31T23:36:28.660016Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We fail to reject the null hypothesis as our p value is greater than the significance level of 0.05 which means \\nthat the amount money company spends is not dependent on Gender missingness. It does not mean that the company pays \\nmore when targeting all genders.'"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''We want to see if the missingness of Gender(NaN = all genders) is dependent on how much a Company spends, aka\n",
    "does a company spend more when they want to target both Genders'''\n",
    "bofa['G_isnull'] = bofa['Gender'].isnull()\n",
    "null = bofa[bofa['G_isnull'] == True]['Spend'].mean()\n",
    "not_null = bofa[bofa['G_isnull'] == False]['Spend'].mean()\n",
    "obs = null - not_null\n",
    "n_reps = 1000\n",
    "diffs = []\n",
    "for _ in range(n_reps):\n",
    "    shuffled = bofa['G_isnull'].sample(replace=False, frac=1).reset_index(drop=True)\n",
    "    bofa['G_shuffled'] = shuffled\n",
    "    shuffled_null = bofa[bofa['G_shuffled'] == True]['Spend'].mean()\n",
    "    shuffled_not_null = bofa[bofa['G_shuffled'] == False]['Spend'].mean()\n",
    "    diff = shuffled_null - shuffled_not_null\n",
    "    diffs.append(diff)\n",
    "p_val = np.count_nonzero(pd.Series(diffs) >= obs) / n_reps\n",
    "'''We fail to reject the null hypothesis as our p value is greater than the significance level of 0.05 which means \n",
    "that the amount money company spends is not dependent on Gender missingness. It does not mean that the company pays \n",
    "more when targeting all genders.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.009"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bofa['Seg_isnull'] = bofa['Segments'].isnull()\n",
    "null0 = bofa[bofa['Seg_isnull'] == True]['Spend'].mean()\n",
    "not_null0 = bofa[bofa['Seg_isnull'] == False]['Spend'].mean()\n",
    "obs0 = null0 - not_null0\n",
    "n_reps = 1000\n",
    "diffs0 = []\n",
    "for _ in range(n_reps):\n",
    "    shuffled = bofa['Seg_isnull'].sample(replace=False, frac=1).reset_index(drop=True)\n",
    "    bofa['Seg_shuffled'] = shuffled\n",
    "    shuffled_null = bofa[bofa['Seg_shuffled'] == True]['Spend'].mean()\n",
    "    shuffled_not_null = bofa[bofa['Seg_shuffled'] == False]['Spend'].mean()\n",
    "    diff = shuffled_null - shuffled_not_null\n",
    "    diffs0.append(diff)\n",
    "p_val0 = np.count_nonzero(pd.Series(diffs0) <= obs0) / n_reps\n",
    "p_val0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.666489Z",
     "start_time": "2019-10-31T23:36:28.664381Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.511"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''We fail to reject the null hypothesis as our p value is greater than the significance level of 0.05 which means\n",
    " that gender has no influence for Impressions per Dollar'''\n",
    "obs = bofa.groupby('Gender')['ImpPerDoll'].mean()\n",
    "obs = abs(obs.diff().iloc[-1])\n",
    "n_reps = 1000\n",
    "diffs = []\n",
    "for i in range(n_reps):\n",
    "    simulation = bofa.sample(replace = False, n = 1000)\n",
    "    diff = simulation.groupby('Gender')['ImpPerDoll'].mean().diff().iloc[-1]\n",
    "    diffs.append(abs(diff))\n",
    "p_val = np.mean(pd.Series(diffs) >= obs)\n",
    "p_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.496"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''We fail to reject the null hypothesis as our p value is greater than the significance level of 0.05 which means\n",
    " that Language has no influence for Impressions per Dollar'''\n",
    "obs2 = bofa.groupby('Agnostic')['ImpPerDoll'].mean()\n",
    "obs2 = abs(obs2.diff().iloc[-1])\n",
    "diffs = []\n",
    "for i in range(n_reps):\n",
    "    simulation = bofa.sample(replace = False, n = 1000)\n",
    "    diff = simulation.groupby('Agnostic')['ImpPerDoll'].mean().diff().iloc[-1]\n",
    "    diffs.append(abs(diff))\n",
    "p_val2 = np.mean(pd.Series(diffs) >= obs2)\n",
    "p_val2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.493"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''We fail to reject the null hypothesis as our p value is greater than the significance level of 0.05 which means\n",
    " that English Language has no influence for Impressions per Dollar'''\n",
    "obs3 = bofa.groupby('English')['ImpPerDoll'].mean()\n",
    "obs3 = abs(obs3.diff().iloc[-1])\n",
    "diffs = []\n",
    "for i in range(n_reps):\n",
    "    simulation = bofa.sample(replace = False, n = 1000)\n",
    "    diff = simulation.groupby('English')['ImpPerDoll'].mean().diff().iloc[-1]\n",
    "    diffs.append(abs(diff))\n",
    "p_val3 = np.mean(pd.Series(diffs) >= obs3)\n",
    "p_val3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.459"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''We fail to reject the null hypothesis as our p value is greater than the significance level of 0.05 which means\n",
    " that OS has no influence for Impressions per Dollar'''\n",
    "obs4 = bofa.groupby('AllOsTypes')['ImpPerDoll'].mean()\n",
    "obs4 = abs(obs4.diff().iloc[-1])\n",
    "diffs = []\n",
    "for i in range(n_reps):\n",
    "    simulation = bofa.sample(replace = False, n = 1000)\n",
    "    diff = simulation.groupby('AllOsTypes')['ImpPerDoll'].mean().diff().iloc[-1]\n",
    "    diffs.append(diff)\n",
    "p_val4 = np.mean(pd.Series(diffs) >= obs4)\n",
    "p_val4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.505"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''We fail to reject the null hypothesis as our p value is greater than the significance level of 0.05 which means\n",
    " that Interests has no influence for Impressions per Dollar'''\n",
    "obs5 = bofa.groupby('All_Interests')['ImpPerDoll'].mean()\n",
    "obs5 = abs(obs5.diff().iloc[-1])\n",
    "diffs = []\n",
    "for i in range(n_reps):\n",
    "    simulation = bofa.sample(replace = False, n = 1000)\n",
    "    diff = simulation.groupby('All_Interests')['ImpPerDoll'].mean().diff().iloc[-1]\n",
    "    diffs.append(abs(diff))\n",
    "p_val5 = np.mean(pd.Series(diffs) >= obs5)\n",
    "p_val5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.473"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''We fail to reject the null hypothesis as our p value is greater than the significance level of 0.05 which means\n",
    " that Age Range has no influence for Impressions per Dollar'''\n",
    "obs6 = bofa.groupby('All_Ages')['ImpPerDoll'].mean()\n",
    "obs6 = abs(obs6.diff().iloc[-1])\n",
    "diffs = []\n",
    "for i in range(n_reps):\n",
    "    simulation = bofa.sample(replace = False, n = 1000)\n",
    "    diff = simulation.groupby('All_Ages')['ImpPerDoll'].mean().diff().iloc[-1]\n",
    "    diffs.append(abs(diff))\n",
    "p_val6 = np.mean(pd.Series(diffs) >= obs6)\n",
    "p_val6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No one put a value for LatLongRad'"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs7 = bofa.groupby('All_Lat')['ImpPerDoll'].mean()\n",
    "obs7 = abs(obs7.diff().iloc[-1])\n",
    "diffs = []\n",
    "for i in range(n_reps):\n",
    "    simulation = bofa.sample(replace = False, n = 1000)\n",
    "    diff = simulation.groupby('All_Lat')['ImpPerDoll'].mean().diff().iloc[-1]\n",
    "    diffs.append(abs(diff))\n",
    "p_val7 = np.mean(pd.Series(diffs) >= obs7)\n",
    "p_val7\n",
    "'''No one put a value for LatLongRad'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.543"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''We fail to reject the null hypothesis as our p value is greater than the significance level of 0.05 which means\n",
    " that Candidate/Ballot has no influence for Impressions per Dollar'''\n",
    "obs8 = bofa.groupby('Unaffiliated')['ImpPerDoll'].mean()\n",
    "obs8 = (obs8.diff().iloc[-1])\n",
    "diffs = []\n",
    "for i in range(n_reps):\n",
    "    simulation = bofa.sample(replace = True, n = 1000)\n",
    "    diff = simulation.groupby('Unaffiliated')['ImpPerDoll'].mean().diff().iloc[-1]\n",
    "    diffs.append((diff))\n",
    "p_val8 = np.mean(pd.Series(diffs) >= obs8)\n",
    "p_val8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''We fail to reject the null hypothesis as our p value is greater than the significance level of 0.05 which means\n",
    " that Districts has no influence for Impressions per Dollar'''\n",
    "obs9 = bofa.groupby('All_Dist')['ImpPerDoll'].mean()\n",
    "obs9 = (obs9.diff().iloc[-1])\n",
    "diffs = []\n",
    "for i in range(n_reps):\n",
    "    simulation = bofa.sample(replace = True, n = 1000)\n",
    "    diff = simulation.groupby('All_Dist')['ImpPerDoll'].mean().diff().iloc[-1]\n",
    "    diffs.append((diff))\n",
    "p_val9 = np.mean(pd.Series(diffs) >= obs9)\n",
    "p_val9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.494"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''We fail to reject the null hypothesis as our p value is greater than the significance level of 0.05 which means\n",
    " that Segments has no influence for Impressions per Dollar'''\n",
    "obs10 = bofa.groupby('Seg')['ImpPerDoll'].mean()\n",
    "obs10 = (obs10.diff().iloc[-1])\n",
    "diffs = []\n",
    "for i in range(n_reps):\n",
    "    simulation = bofa.sample(replace = True, n = 1000)\n",
    "    diff = simulation.groupby('Seg')['ImpPerDoll'].mean().diff().iloc[-1]\n",
    "    diffs.append((diff))\n",
    "p_val10 = np.mean(pd.Series(diffs) >= obs10)\n",
    "p_val10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
